{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPaBDDEoZRAU"
      },
      "source": [
        "# Chapter 3: Ethics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_5DN9_ydMJH"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh0RXQwwY2Nz",
        "outputId": "4182836e-cec0-4238-c3e0-3f669c1158d7"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM07oPfeZVdk"
      },
      "source": [
        "## Data Ethics "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8LJiCpJdQTf"
      },
      "source": [
        "Three common ethical issues in tech: \n",
        "1. Recourse processes: e.g. bugs in algorithms\n",
        "2. Feedback loops: e.g. predictive policing, recommender systems causing conspiracy theory booms\n",
        "3. Bias: dataset bias leads to biased outputs, e.g. racism\n",
        "\n",
        "Example of bad things that can happen if you ignore the ethical impacts of technology: IBM and the Holocaust."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-JGVSzGGh8h"
      },
      "source": [
        "### Recourse and Accountability\n",
        "- Who is to blame for a bad situation? The algorithm? The designer? The user?\n",
        "- Recourse is necessary for correcting errors in data, and understanding accountbaility for situations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRCCvcDIHO5B"
      },
      "source": [
        "### Feedback loops\n",
        "- Harmful positive feedback loops can form even when the model was never designed to do this - how does this happen?\n",
        "- Part of the probably is reward hacking, where the goal is just to maximise some metric. If there is a flaw in the reward function, then the algorithm exploits it to the utmost. \n",
        "- Humans need not be in the loop! e.g. if a Youtube video is initially misclassified by the algorithm, then future classifications of videos that *use this as an example* become more likely to misclassify things as well\n",
        "\n",
        "\n",
        "To break feedback loops, there are several things that can be done: \n",
        "- Remove the feature in question (e.g. sex or gender) from the optimisation algorithm\n",
        "- This can be done in anticipation, or to take action to break a loop once it shows itself\n",
        "\n",
        "Feedback loops can also interact with bias in troublesome ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7uGbbHyKFuL"
      },
      "source": [
        "### Bias (social science)\n",
        "Here we consider four types of bias, in the social science sense.\n",
        "\n",
        "1. Historical bias: people, processes and society are biased - this can cause data to become biased, which leads to the model predictions being biased\n",
        "\n",
        "2. Measurement bias: we might measure the wrong thing, or measure things in the wrong way, leading to mistakes in the model. e.g. does colonoscopy cause strokes? No! But getting colonoscopies is correlated with going to the doctor when you have a stroke.\n",
        "\n",
        "3. Aggregation bias: the model doesn't aggregate data in a way that incorporates all the appropriate factors, e.g. in some specific cases, a correct diagnosis may depend strongly on a person's genetic profile, but the model fails to consider this\n",
        "\n",
        "4. Representation bias: the model *amplifies* existing biases in the training set, e.g. representation of genders in different occupations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DW0o47lMyv7"
      },
      "source": [
        "### Algorithmic bias vs human bias\n",
        "Different types of bias need different mitigation approaches. \n",
        "\n",
        "Also, AI bias isn't \"just a human problem\" in the sense that \"humans are bias so AI is biased\". There are factors inherent to AI that make them different: \n",
        "- ML can create feedback loops\n",
        "- ML can amplify bias\n",
        "- Algorithms and humans are used differently\n",
        "- Technology is power\n",
        "\n",
        "Furthermore, algorithms are used quite differently compared to human decision makers\n",
        "- People are more likely to assume algorithms are objective\n",
        "- Algorithms are more likely to be implemented with no appeals process in place\n",
        "- Algorithms are often used at scale\n",
        "- Algorithmic systems are cheap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKmyZ-YsNl9J"
      },
      "source": [
        "### Disinformation\n",
        "Negative impacts can still arise even without bias, e.g. when algorithms are used for disinformation. \n",
        "\n",
        "Disinformation is not just about false information - it often contains true information, or half-truths that are taken out of context. \n",
        "\n",
        "This often involves coordinated campaigns, and can also be enhanced using generated text. \n",
        "\n",
        "To combat this, we can try using digital signatures to verify digital content. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6jsGwMQOdM6"
      },
      "source": [
        "## Addressing Ethical Issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aICadwtPO1uM"
      },
      "source": [
        "### Analysing a project\n",
        "Throughout development of a project, you should be considering ethical implications. Ask the following questions: \n",
        "- Should we even be doing this?\n",
        "- What bias is in the data?\n",
        "- Can the code and data be audited?\n",
        "- What are the error rates for different sub-groups?\n",
        "- What is the accuracy of a simple rule-based alternative?\n",
        "- What processes are in place to handle appeals or mistakes?\n",
        "- How diverse is the team that built it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBKXGvCjPc28"
      },
      "source": [
        "### Implementing ethical checks\n",
        "In a company or group, you should consider: \n",
        "- Whose interests/skills/experiences/values have we assumed rather than actually consulted?\n",
        "- Who are the stakeholders who will be directly affected by our project? Have we actually protected their interests?\n",
        "- Who will be indirectly affected in significant ways?\n",
        "- Who might use this product ways we didn't expect?\n",
        "\n",
        "Ethical lenses help us find issues, by considering different philosophical views. e.g. we might consider the issues from a utilitarian or deontological perspective. You can check whether an ethical lens is complete by first coming up with a bunch of processes/questions to resolve some problem. Then try to come up with an example that is something that no one would think is acceptable - this helps refine the solution. \n",
        "\n",
        "Increasing diversity of teams can help consider more perspectives, finding better solutions, identify problems sooner, and considering a wider range of solutions. \n",
        "\n",
        "Fairness, accountability and transparency are very important problems not just in the technical domain. We therefore shouldn't just think about short-term technical fixes; we need to consider the wider social problem. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGYHsnGCSE1S"
      },
      "source": [
        "### Policy\n",
        "Regulation via financial penalties can have a very large effect, perhaps even more so than ethical problems (unfortunately). e.g. Facebook responded much more quickly to the German law against hate speech, rather than the destruction of ethnic minorities (in Myanmar and Rwanda).\n",
        "\n",
        "But the law will never cover all edge cases, and so software developers/data scientists need to make ethical decisions in practice. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9fYlKPmTFDr"
      },
      "source": [
        "## Questionnaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLjQkchVq-dv"
      },
      "source": [
        "1. **Does ethics provide a list of \"right answers\"?**  \n",
        "Not really. It's about \"right\" and \"wrong\", but things are still pretty unclear. \n",
        "\n",
        "1. **How can working with people of different backgrounds help when considering ethical questions?**  \n",
        "People have different ethical views, and considering a wider range of backgrounds makes it more likely that the outcome of the collaboration will be some that is more inclusive and representative of these views. \n",
        "\n",
        "1. **What was the role of IBM in Nazi Germany? Why did the company participate as it did? Why did the workers participate?**  \n",
        "IBM made it easier for the Nazis to identify \"undesirable\" ethnic groups, and perform the Holocaust. They were mostly following orders. \n",
        "\n",
        "1. **What was the role of the first person jailed in the Volkswagen diesel scandal?**  \n",
        "An engineer who was doing what they were told.\n",
        "\n",
        "1. **What was the problem with a database of suspected gang members maintained by California law enforcement officials?**  \n",
        "It was full of errors, including babies less than 1 year old who \"admitted to being gang members\".\n",
        "\n",
        "1. **Why did YouTube's recommendation algorithm recommend videos of partially clothed children to pedophiles, even though no employee at Google had programmed this feature?**  \n",
        "The original video had revealing frames for brief periods, but when it gained attention from some pedophiles, the algorithm started recommending to more people who watched the same content. \n",
        "\n",
        "1. **What are the problems with the centrality of metrics?**  \n",
        "If the main goal is *just* to \"maximise some metric\", and there are ways to exploit it, then the model will do so. This can lead to unintended consequences, if the metric isn't representative of your actual preferences. \n",
        "\n",
        "1. **Why did Meetup.com not include gender in its recommendation system for tech meetups?**  \n",
        "Men seemed to express more interest than women in tech meetups, so this might result in the algorithm recommending the meetups more to men than women, leading to a feedback loop. \n",
        "\n",
        "1. **What are the six types of bias in machine learning, according to Suresh and Guttag?**  \n",
        "Historical bias, representation bias, measurement bias, evaluation bias, aggregation bias, deployment bias\n",
        "\n",
        "1. **Give two examples of historical race bias in the US.**  \n",
        "Doctors were less likely to recommend helpful medical procedures to Black patients (when given identical files). Blacks were offered higher prices when bargaining for cars. \n",
        "\n",
        "1. **Where are most images in ImageNet from?**  \n",
        "US and Western countries. So models trained on ImageNet tend to do worse on things from other countries.\n",
        "\n",
        "1. **In the paper [\"Does Machine Learning Automate Moral Hazard and Error\"](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf) why is sinusitis found to be predictive of a stroke?**  \n",
        "A person who is more likely to visit the doctor from sinusitus is also more likely to visit the doctor when they get a stroke. Correlation, but not causation.\n",
        "\n",
        "1. **What is representation bias?**  \n",
        "An imabalance of representation of different groups in a particular role, leading to the models amplifying the existing bias. \n",
        "\n",
        "1. **How are machines and people different, in terms of their use for making decisions?**  \n",
        "Machines can create feedback loops, amplify bias, are used at larger scale for cheaper and with fewer appeals processes.\n",
        "\n",
        "1. **Is disinformation the same as \"fake news\"?**  \n",
        "No. Disinformation can also include half-truths, or truths taken out of context.\n",
        "\n",
        "1. **Why is disinformation through auto-generated text a particularly significant issue?**  \n",
        "It's hard to determine accountability, and hard to prevent. Furthermore it seems to be exacerbated by deep learning. \n",
        "\n",
        "1. **What are the five ethical lenses described by the Markkula Center?**  \n",
        "Rights (respecting rights), justice (treating people proportionately), utilitarian (doing the most good), common good (best serving the whole community), virtue (most virtuous actions)\n",
        "\n",
        "1. **Where is policy an appropriate tool for addressing data ethics issues?**  \n",
        "Where incentives are small otherwise, and potentially to help address underlying issues (rather than rely on small tech-based fixes)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "fastai_03_ethics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
